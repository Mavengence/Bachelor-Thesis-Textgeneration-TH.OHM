\chapter{Prototype}\label{ch:proto}

In this third section of my thesis I finally present my self programmed prototype. The State of the Art chapter was structured in such a way, that it focuses on the necessary information for understanding my prototype. Hence I only gave some additional information about techniques which produce even more accurate results, but this is out of scope for my thesis. This section is divided into four sections, starting with the objective (Section \ref{ss:obj}) of this prototype. I propose my requirements for this prototype and what I expect to achieve from it. The next step is the technical concept (Section \ref{ss:tc}), which models the data flows and processes inside the program. After I explain which steps my algorithm goes through, I present snippets of my code in Section \ref{ss:imp} to further illustrate it. This code will be evaluated in Section \ref{ss:eval}.

\section{Objective}\label{ss:obj}

\textit{"I don't have time to read the entire report, please give me a brief summary"}. Many years ago the challenge was to find the right information for a specific tasks, nowadays the internet provides more information than anyone could ever read. Depending on the type of document or article, it is not always necessary to read it from the beginning to the end. Throughout my thesis I often used my case study \textit{News Headline Summarization from Google} for illustrating certain concepts. I chose a different topic for my case study than my actual prototype to show another kind of task which automatic text summarization is capable of.

I chose the \textbf{Amazon Fine Food Reviews} provided from \textit{Stanford Network Analysis Project} hosted on the website Kaggle \footnote{https://www.kaggle.com/snap/amazon-fine-food-reviews/}. This website is commonly known for providing data-sets from private and organizational publishers. 

The project's objective is to build and train a model that can compute relevant summaries for reviews written about fine foods sold on Amazon. 

This dataset contains around 500.000 entries with each entry containing one review from the amazon fine foods and its summary. The data is collected for 8 years from October 2012 until now. 

\section{Technical concept}\label{ss:tc}

An example review needs to pass from the given data-set file to the predicted summary a lot of processes and transformations. The technical concept illustrates all necessary steps without yet going into the programming detail itself. The purpose of this section is to take all the previous explanations into a combined example and show the entire process from the beginning to the end. 

The processing steps follow in chronological order:

\begin{itemize}
	\item Data pre-processing
	\item Build the Model
	\item Train the Model
	\item Generating Summary
\end{itemize}

This steps will be explained in the following Subsection.

\subsection{Data Pre-processing}

The computer does not understand words. It is not possible to feed the words from a review subsequently into a recurrent neural network and expect that the algorithm will learn it somehow. Performing pre-processing is a crucial step before feeding inputs into the neural network model. The data must be normalized into a clean and not messy way. The following is an example from the original dataset:

\begin{tcolorbox}
	\textbf{Review} \\
	I can remember buying this candy as a kid and the quality hasn't dropped in all these years. Still a superb product you won't be disappointed with. \\
	
	\textbf{Summary} \\
	Delicious product!
\end{tcolorbox}

For all the words occurring in the review and summary a vocabulary dictionary needs to be created. This dictionary can be used to convert the words into numbers and respectively numbers is what the recurrent neural network wants as an input. For creating the two vocabulary dictionaries it needs to be carefully though of how to handle the data. If the same word e.g. \textit{Still} and \text{still} is once with a capital letter and the second time without, the encoding (\textit{UTF-8}) is different and hence the computer regards this as two different words. Furthermore the text can include punctuation, quotation marks or parenthesis. Therefore I defined a chronological order of text transformation steps to normalize the text:

\begin{tcolorbox}
	\begin{itemize}
		\item Convert all words into lowercase
		\item Remove occurring HTML tags
		\item Contraction mapping
		\item Remove (‘s)
		\item Remove any text inside the parenthesis ( )
		\item Eliminate punctuations and special characters
		\item Remove stopwords
		\item Remove short words
	\end{itemize}
\end{tcolorbox}

\textbf{Contraction mapping} describes the process of mapping contracted words into their original two words. For example \textit{haven't} will be mapped back to \textit{have not}. This is important, because only this way the algorithm can detect the word \textit{not} to be negative connotated. If a review says something is not bad, then the algorithm can summarize it by computing the word good. This relation can only be learned by the algorithm by providing it all the necessary words.

\textbf{Remove stopwords} limits the vocabulary size, which reduces the models training time dramatically. I have already explained stopwords in Section \ref{ss:sd}. Those are words like and, that or there. 

\textbf{Remove short words}, because they most likely don't play a vital role in predicting the output. Words like it, in or at not necessarily appear in the stoowords dictionary. For this reason I remove them manually, because they almost don't affect the text's meaning, but after removing the reduce the training time further. 

After applying all of the pre-processing steps, the example will look as the following:

\begin{tcolorbox}
	\textbf{Cleaned Review} \\
	remember buying candy kid quality dropped years still superb product disappointed \\
	
	\textbf{Cleaned Summary} \\
	delicious product
\end{tcolorbox}

The data is now clean and normalized. Furthermore it can be check whether some data is duplicated and remove this data respectively. 

\begin{figure}
	\begin{center}
		\includegraphics[width=6in]{photos/dist_seq}\\
		\caption{Distribution of the sequences to estimate the maximum length for the review and summary}\label{dist_seq}
	\end{center}
\end{figure}

In order to build the mentioned vocabulary, the text needs to be \textbf{Tokenized}. This means selecting all the single words of a sentence and store all the words uniquely in the vocabulary for the review and the other vocabulary for the summary. The only thing left is to compare the word counts of all sentences and take the median value for the maximum review length and the maximum summary length as shown in Figure \ref{dist_seq}. This is important, because a fixed length is mandatory for building the model. All of the words are now replaced by their dictionary number representation and if a review is shorter as the maximum length, the missing data points are filled with zeros.

\begin{tcolorbox}
	\textbf{Tokenized and Padded Review}
	
	array([[ 678,  142,  360, 1017,   87, 3724,  108,   55, 1893,    9,  243,
	0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
	0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
	0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
	0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
	0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
	0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
	0,    0,    0]], dtype=int32) \\
	
	\textbf{Tokenized and Padded Summary}
	
	array([[ 1, 21, 19,  2,  0,  0,  0,  0,  0,  0]], dtype=int32)
\end{tcolorbox}

This is the necessary format for building up a model.

\subsection{Building the Model}

The model follows an abstractive approach (Section \ref{ss:abstractive}) and is based on the Advanced Approaches for Text Generation (Section \ref{ss:aatg}). Since the objective is to build a text summarizer, the input is the fixed length sequence of words and the output is a shorter, but also fixed length sequence of words (Figure \ref{build_model}).
	
The mathematical notations for Figure \ref{build_model} are shown in the box below.
	
\begin{tcolorbox}
	\boldmath{\([x_{1}, x_{2}, x_{3}, ..., {x_{Tx}]}\)} where each \textit{x} represents one word of the input sequence, \\ e.g. \(x_{1}\) = remember \\ \\
	\boldmath{\([y_{1}, y_{2}, x_{3}, ..., {y_{Ty}]}\)} where each \textit{x} represents one word of the input sequence, \\ e.g. \(y_{1}\) = delicious \\ \\
	\boldmath{\(T_{x}\)} represents the fixed length of the input sequence \\ \\
	\boldmath{\(T_{y}\)} represents the fixed length of the output sequence
\end{tcolorbox}

The reason for choosing the LSTM over a basic recurrent neural network architecture is, that the generated summary can be sometimes quite long (up to 20 words) and the recurrent neural network cannot catch up with dependencies of this sequence length. Figure \ref{modelenc} illustrates the overall view of my model. 
As explained in the last section, if a input or output has less words than the maximum length, the missing words are denoted as zero values. This allows the algorithm to not compute any further prediction, because it is a zero multiplication. The encoder decoder architecture is mainly used when the input and output length vary from each other. 

\begin{figure}
	\begin{center}
		\includegraphics[width=6in]{photos/build-1}\\
		\caption{Many to many Sequence to Sequence LSTM model}\label{build_model}
	\end{center}
\end{figure}

Figure \ref{build_model} shows the general sequence to sequence model, whereas Figure \ref{modelenc} illustrates the perspective of the text summarizer. 

\begin{figure}
	\begin{center}
		\includegraphics[width=3.5in]{photos/modelenc-1}\\
		\caption{Perspective of the text summarization}\label{modelenc}
	\end{center}
\end{figure}

The mathematical notations for Figure \ref{modelenc} are shown in the box below.

\begin{tcolorbox}
	\boldmath{\(h\)} represents the input (update) gate, which is responsible for adding new information into the cell \\ \\
	\boldmath{\(c\)} represents the cell state, which is the horizontal line between multiple LSTM cells
\end{tcolorbox}

\subsection{Training the Model}

For the training phase, first of all the encoder and decoder need to be set up individually from each other. The model predicts the next word based on the current word (or the start). To achieve this, the prediction is one time step in the future. 

The \textbf{Encoder} in Figure \ref{trainenc} uses LSTM cells to read all of the sentences from the data-set in. Each sentence will be read in at a time and further each word of the sentence will be read for each at a time step. Therefore the LSTM has a one word input per time step and captures the contextual information of the sentence out of it.  The \textit{h} and \textit{c} in Figure \ref{trainenc} are the same as the ones from the previous Figure \ref{modelenc}. 

\begin{figure}
	\begin{center}
		\includegraphics[width=6in]{photos/trainenc-1}\\
		\caption{Encoder of the LSTM}\label{trainenc}
	\end{center}
\end{figure}

The encoder and the decoder are two different LSTM architectures. The initial state of the encoder LSTM is a zero matrix and the finally computed output state of the encoder LSTM is the initial state of the decoder LSTM.

The \textbf{Decoder} is based on a LSTM network as well, which has the target sequence as a word-by-word input and predicts the same sequence offset by one time step. The decoder is trained to predict the upcoming word in the sequence with respect to the previous word. The \textit{start} and \textit{end} boxes in Figure \ref{traindec} represent an appended string to the beginning and ending of the original summary. This is done to create the one time step offset.

\begin{figure}
	\begin{center}
		\includegraphics[width=6in]{photos/traindec-1}\\
		\caption{Decoder of the LSTM}\label{traindec}
	\end{center}
\end{figure}

The target sequence is unknown during the decoding part of the test sequence. Target sequence is predicted by passing the first word into the decoder which would is always the \textit{start} token. And the \textit{end} token signals the end of the sentence, followed by potentially zero values.


\subsection{Generate the Summary}

This part is also known as the \textit{Inference Phase}. After training on a selected amount of reviews and summaries, the model is tested on new source sequences where the target sequence is unknown. For this to work,  a inference architecture to decode a test sequence is necessary. The inference goes through various processes:

\begin{itemize}
	\item 1) Encode the review and initialize the decoder with internal states of the encoder
	\item 2) Pass the \textit{start} token as an input to the decoder
	\item 3) Compute the decoder for one time step with the stored internal states
	\item 4) The output at each time step is the probability for the next word. The maximum probability word will be selected
	\item 5) Pass the selected word as an input to the next decoder time step and update the internal states with the current time step
	\item 6) Repeat steps \textit{3} – \textit{5} until the \textit{end} token is generated with the highest probability
\end{itemize}

Figure \ref{infer} shows the architecture for the inference phase. The test sentence goes through all the initially trained weights (states h, c) of the model. There is no more training necessary, just the calculations of the test sequence with the internal state vectors to predict the output sequence. After a certain prediction length, the probability for the \textit{end} token to occur is the highest and if there is remaining space for maximum summary length, it will be filled up with zero values again. 

\begin{figure}
	\begin{center}
		\includegraphics[width=5in]{photos/infer-1}\\
		\caption{Inference architecture to decode a sequence}\label{infer}
	\end{center}
\end{figure}

Even a LSTM cell has limitations when it comes to sentence length. Since summaries can be quite long (my maximum length is 80 words), there need to be made further improvements for the algorithm in order to work properly. It is difficult for the encoder to memorize long sequences and output a fixed length vector. For this reason I have introduced the attention mechanism in Section \ref{ss:atten}. The intuition for attention is:

\begin{tcolorbox}
	\textit{How much attention do we need to pay to every single word in the input sequence for generating a new word at time step \textbf{t}? That is the primal intuition behind the attention mechanism.}
\end{tcolorbox}

With this mechanism it is possible for the decoder to first look on all the words from the encoder output and decide to which parts of the sentence the decoder wants to focus time-by-time. The attention model is placed on top of the LSTM cell and requires additional computation. For my model I am using the \textbf{global attention model} from \cite{effectiveattn} shown in Figure \ref{glob}, where \textit{h} denotes the same weights as Figure \ref{modelenc} and \textit{a} is the attention parameter, calculated for every word at every time step \textit{t}.

\begin{tcolorbox}
	\textbf{h} is the hidden state trained on the encoder \\ \\
	\textbf{t} is the time step which represents always a single word \\ \\
	\textbf{a} is calculated \boldmath{\(T_{y}\)} times at every time step \textbf{t}, so for the attention model we have \textbf{a} * \boldmath{\(T_{y}\)} * \textbf{t} new parameters
\end{tcolorbox}

\begin{figure}
	\begin{center}
		\includegraphics[width=3.5in]{photos/glob_att}\\
		\caption{Global Attention Model from \cite{effectiveattn}}\label{glob}
	\end{center}
\end{figure}

\section{Implementation}\label{ss:imp}

I will not provide the entire code in this section, but I will introduce the most crucial aspects of the code. For this project, the following python packages are necessary: 

\begin{lstlisting}[language=Python,caption={Python package dependencies},captionpos=b]
import numpy as np  
import pandas as pd 
import re           
from bs4 import BeautifulSoup 
from keras.preprocessing.text import Tokenizer 
from keras.preprocessing.sequence import pad_sequences
from nltk.corpus import stopwords   
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
\end{lstlisting}

I used python version 3.7.6 and I programmed it entirely in the \textbf{Anaconda} IDE (Integrated Development Environment).
Most importantly to say is, that numpy is responsible for the computational part, because it computes on a very low level, which saves up a lot computing time. Pandas is responsible for reading in the data-set and preparing it in such a way, that it is in the correct normalized form as explained to feed it into the network.

\begin{lstlisting}[language=Python,caption={Python package dependencies},captionpos=b]
# Encoder 
encoder_inputs = Input(shape=(max_len_text,)) 
enc_emb = Embedding(x_voc_size, latent_dim,trainable=True)(encoder_inputs) 

#LSTM 1 
encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) 
encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) 

#LSTM 2 
encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) 
encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) 

#LSTM 3 
encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) 
encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) 

# Set up the decoder. 
decoder_inputs = Input(shape=(None,)) 
dec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True) 
dec_emb = dec_emb_layer(decoder_inputs) 

#LSTM using encoder_states as initial state
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) 
decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) 

#Attention Layer
Attention layer attn_layer = AttentionLayer(name='attention_layer') 
attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) 

# Concat attention output and decoder LSTM output 
decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])

#Dense layer
decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax')) 
decoder_outputs = decoder_dense(decoder_concat_input) 

# Define the model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs) 
\end{lstlisting}

%You can also include listings from a file directly:

%\lstinputlisting[language=Python,caption={This is an example of included listing},captionpos=b]{listings/example.py}

\section{Evaluation}\label{ss:eval}

\textbf{Loss}

\begin{figure}
	\begin{center}
		\includegraphics[width=3.5in]{photos/eval}\\
		\caption{Training and validation loss during the training phase}\label{eval}
	\end{center}
\end{figure}


\begin{figure}
	\begin{center}
		\includegraphics[width=6in]{photos/model_sum}\\
		\caption{Model summarization with parameters}\label{model_sum}
	\end{center}
\end{figure}

\textbf{Predictions}

\begin{table}[]
	\begin{tabular}{@{}cccl@{}}
		\toprule
		\textbf{Cleaned Review}                                                                                                                                                           & \textbf{Cleaned Summary}                                                                   & \multicolumn{1}{l}{\textbf{Generated Summary}} &  \\ \midrule
		\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}ordered chips found salty \\ dry huge amount spices \\ ball one bags opened\end{tabular}}                                         & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}too salty \\ and dry\end{tabular}}          & \multicolumn{1}{c|}{too salty}                 &  \\ \cmidrule(r){1-3}
		\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}found tea favorite movie theater\\  found perfect tea guests \\ everyone loves makes love\end{tabular}}                           & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}at the movies \\ and home\end{tabular}}     & \multicolumn{1}{c|}{love it}                   &  \\ \cmidrule(r){1-3}
		\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}dogs special diet treats\\  feed favorites cause problems\end{tabular}}                                                           & \multicolumn{1}{c|}{must be good}                                                          & \multicolumn{1}{c|}{my dogs love these}        &  \\ \cmidrule(r){1-3}
		\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}delicious sherry flavor salad \\ dressing great used marinade\\ give try sweet balsamic \\ tart red wine vinegar\end{tabular}}    & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}yummy sweet \\ sherry vinegar\end{tabular}} & \multicolumn{1}{c|}{love these}                &  \\ \cmidrule(r){1-3}
		\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}received medium roast receive \\ correct coffee shown picture \\ disappointed suppose\\  ill try lot trouble return\end{tabular}} & \multicolumn{1}{c|}{wrong coffee received}                                                 & \multicolumn{1}{c|}{coffee received}           &  \\ \bottomrule
	\end{tabular} \\ \\
	\caption{This table shows some example outputs for the training data from my trained model} 
\end{table}

