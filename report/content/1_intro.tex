\chapter{Introduction}\label{ch:intro}

The 21st century is flushed with an massive amount of texts and documents. Every day there are new articles, news, documentations and reports packed with information. For this reason, a new discipline, especially for students like me arose out of this. Knowledge is nowadays accessible everywhere and immediately, but the consumption takes way too much time. Websites like \textit{https://www.blinkist.com/de} provide their costumers text summarizations of different kind of books readable in 15-30 minutes. This is an interesting way to safe time, but still this summarizations is done by hand. Artificial Intelligence researchers continuously provide knowledge to the public to summarize text with computer algorithms. The first approaches of automatic text summarization were grammatically bad and no human wants to read a grammatically broken summarizations. Bt Deep Learning changed the game completely, because algorithms are now feasible enough to summarize texts as good as humans. 

\section{Structure of the thesis}
The aim of my thesis is to survey the current state of the art in text generation, especially on the focus of text summarization. The development into the state of the art neural text summarization had a huge impact on the readability for the human. For readers who are not familiar with machine learning in general, I will provide a zoom-in introduction from artificial intelligence in general into the tiny sub field text summarization. My approach is feed-forward from the definition of machine learning, deeper into the natural language processing field, further into the text generation field and within that, I focus on the text summarization part in chapter 1 - Introduction. New research and state of the art results in some natural language processing fields often lead to improvements across other related disciplines in machine learning and natural language processing, because algorithms are sometimes usable vice-versa. For this reason, I provide the most crucial text generation historical achievements in combination with the latest text summarization results, because both topics intersect in many aspects. The crucial concept of historical and modern approaches to summarize and generate text are introduced in chapter 2 - An evolutionary view on the State of the Art. 
To illustrate the basic workflow of a text summarizing system, I programmed a prototype. The concept, development and evaluation of this summarizer are located in chapter 4 - Prototype, but it requires prior knowledge to fully understand the mechanism from the input to the output. Finally in the last chapter I will discuss further improvements for my prototype and a brief discussing of future of text generation.

\section{Machine Learning}
In the last decade, Machine Learning (ML) is increasingly finding its way into businesses and society. Many websites and businesses use Machine Learning techniques to improve the user and costumer experience. The phrase \textit{Machine Learning} was originally introduced in 1952 by Arthur Samuel. He developed a computer program for playing the game checkers in the 1950s. Samuel's model was based on a model of brain cell interaction by Donald Hebb from his book called \textit{The Organization of Behavior} published in 1949. Hebb's book introduces theories on neuron excitement and the neural communication. Figure \ref{neuron} illustrates a mathematical approximation of the humans brain cell in form of an \textit{artifical} neuron. Nowadays, this brain-neuron based model is mostly declared to be not realistic enough [Andrew Ng, deeplearning.ai], because the structure of a brains neuron is far more complex than the illustration in figure \ref{neuron} suggests. Nevertheless, it provides a really good entry point for this research field in my opinion. 

\begin{figure}
  \begin{center}
  \includegraphics[width=3.5in]{photos/neuron}\\
  \caption{A simple Neuron with 3 inputs and 1 output \cite{neuron}}\label{neuron}
  \end{center}
\end{figure}
 
The roots of Neural Networks (NN) lie down almost 80 years ago in 1943 when \textbf{McCulloch-Pitts} \cite{NN} compared for the first time neural networks with the structure of the human brain. The range in which Neural Networks (in the year 2020) apply to modern technologies is wide. Some disciplines have only been created due to the invention of Neural Networks, because they solve existing and new problems more effective and efficient than previously used algorithms. Many frequently held conferences around the globe proof continuous evidence of the successes of Neural Networks. Among those various disciplines counts for example \textit{Pattern recognition} with Convolutional Neural Networks (CNN) \cite{cnn}. Convolutional Neural Networks are a one of the many special building blocks of the neural network. Every building block aims to solve a different task. For example, Pattern recognition uses different layers (building blocks) in its neural network than text summarization, because the input for Pattern recognition neural networks is often a picture consisting of e.g. 32x32 pixels, whereas the input for the text summarization is e.g. a 1000 word long text.  

A widely known entry challenge into pattern recognition is the \textit{CIFAR-10} dataset \cite{cifar}. It consists of 50.000 images divided into 10 classes of different objects and animals like cats and cars (5000 images of cats, 5000 images or cars, ...). Classification algorithms try now to predict a class for the input image as precise as possible. Many amateurs \cite{tim} and experts annually attempt to show their latest results in beating the former best accuracy. 

Natural Language Processing is one of the various sub-fields of Machine Learning. Strictly speaking, it is actually a multidisciplinary field consisting of Artificial Intelligence (AI) and computational linguistics. Natural Language Processing is dedicated to understand and process the interactions between human (natural) language and computers. Natural Language Processing is a very broad term and can be applied on many different tasks, such as: 

\begin{itemize}
\item \textbf{Sentiment Analysis}, e.g. Google Reviews on Restaurants
\item \textbf{Machine Translation}, e.g. Google Translator
\item \textbf{Speech Recognition}, e.g. Siri from Apples IPhone
\item \textbf{Text Generation (Neural Text Generation [\textit{NTG}])}, e.g. Text Summarization
\item \textbf{Chat Bots}, e.g. Shopping Websites
\end{itemize}

\textbf{Deep Learning} is not an absolute definition. Many top researches define it very differently. The core can be broken down to, Deep Learning allows to build more complex neural networks, which are capable of detecting better and more correlation in data. Figure \ref{ai} shows the zoom-in from AI to Deep Learning. Therefore Deep Learning can be seen as a method in Machine Learning, not to mix up with Natural Language processing, which can make use of Deep Learning techniques, but it is not required to.

\begin{figure}
	\begin{center}
		\includegraphics[width=5.5in]{photos/ai_ml_dl.jpg}\\
		\caption{Zoom into Artificial Intelligence from \textit{https://rapidminer.com/blog/artificial-intelligence-machine-learning-deep-learning/}}\label{ai}
	\end{center}
\end{figure}

All of this tasks require many steps to function properly. In the broadest sense, there is always an Input and an Output, which are shown in Table \ref{tab:nlp_table}.

\begin{center} 
	\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}|}
		\hline
		\multicolumn{4}{|c|}{\textbf{Example components of Input - Output systems}}\\ \hline\hline
		&Speech &Text &Images \\ \hline
		Input &Speech &Text &Image \\
		Analysis &Recognition  &Recognition     &Recognition \\ \hline \hline
		Output &Generation &\cellcolor[HTML]{F3E687}Generation &Generation \\
		Synthesis &of Speech & \cellcolor[HTML]{F3E687}of Text &of Images \\ \hline \hline
		Processing &NLP &\cellcolor[HTML]{7ebfd3}NLP &CNN \\
		method &method  &\cellcolor[HTML]{7ebfd3}method     &Building Blocks \\ \hline
	\end{tabular}
	\captionof{table}{A closer look into Input Output systems with the focus on Text Generation}
	\label{tab:nlp_table}
\end{center}

\begin{center} 
	\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}|}
		\hline
		\multicolumn{4}{|c|}{\textbf{Examples of Natural Language Processing systems}}\\ \hline\hline
		&Speech &Text &Text \\ \hline
		Input &Siri &Read in &Read in \\
		Analysis &listens  &document     &document \\ \hline \hline
		Output &Siri &\cellcolor[HTML]{F3E687}Generate &Generate \\
		Synthesis &answers & \cellcolor[HTML]{F3E687}Summary &sentiment \\ \hline
	\end{tabular}
	\captionof{table}{Examples for three different NLP tasks}
	\label{tab:nlp_table2}
\end{center}

It shows that Text Generation is the \textbf{output part} of a \textbf{Natural Language Processing} model. Data is collected through various different sources, e.g. images, videos or speech, then it is further processed and generates the desired output. Useful examples are shown in Table \ref{tab:nlp_table2}.

For this Bachelor thesis, the focus is on the output part of a Natural Language Processing system, more specifically the text summarization ,which inputs text as shown in Table \ref{tab:nlp_table} and \ref{tab:nlp_table2} and outputs the summary. Text generation is therefore in general the output part of an input-output NLP system.

But what defines a summarization? Literature points out multiple different definitions. One definition proposes that the summary of a document is the process of distilling the most important information from a source (or sources) to produce an abridged version for a particular user (or users) and task (or tasks) \cite{def}. Its objective is to give information and provide classified access to the source documents. Summarization is an automatic task, when it is generated by software in general or an algorithm.

Another term for Text Generation is  \textit{Language Modelling}, because text generators use the words of a language and grammar as input for the model. In the past five years, primarily two approaches were used for modeling a Natural Language Processing system, namely the \textbf{rule-based} system and the \textbf{template-based} system (Figure \ref{rules_based}) \cite{NTG2}. Today neural end-to-end systems are \textit{state-of-the-art} \cite{End_to_End}. These systems offer more flexibility and scale with proportionately better results, and less data is required because of the increased complexity. These systems are called neural, because they make use of \textbf{Deep Learning} Neural Networks.
A major disadvantage is that the necessary computing power has increased exponentially. However, this leads to a complex problem because it becomes more and more challenging to understand the decisions of the neural network. The neural network is still, to a large extent, a \textit{black box}. But especially in NLP it gives surprisingly good results. The neural network models for text processing are difficult to understand, so nowadays, compromises between rule-based systems still have to be made, and hybrid systems are most commonly in use. 


\begin{figure}
  \begin{center}
  \includegraphics[width=3.5in]{photos/rule_based}\\
  \caption{Rule-Based vs. Neural-Text-Generations System \cite{NTG2}, Page 4}\label{rules_based}
  \end{center}
\end{figure}

When Neural end-to-end systems are used, Text Generation is often referred as Neural Text Generation (NTG). More examples for Neural Text Generators as output synthetical component are:

\begin{itemize}
\item Speech recording and conversion to text
\item Conversation systems e.g. chatbots
\item Neural Text summary
\item Caption generation of Images
\end{itemize} 

In order to train language models properly, Deep Learning (DL) algorithms teach the model the probabilities of occurring words with respect to the preceding words. There are several approaches to achieve this goal. Language models can be trained on the level of words, whole sentences, or even whole paragraphs. The granularity in which the training takes place is called \textit{n-grams}, where \textit{n} represents the number of preceding words. Further explanation in Section \ref{ss:ngram} of Chapter \ref{ch:sota}. 


\section{Case study of an Automatic Text Summarization System (ATS)}\label{ss:case}

As a human, creating an good summary of a text requires that the person understood the text well. The text needs to be understood so well, that the person can summarize the texts essence in such a way, that it shortens the original document to a minimum down. However, after a period of time the person is most likely to summarize the exact same text in a different way than for example one month ago. Due to this circumstance, the summarization task tends to be challenging to automate. Depending on what kind of summary is needed, the texts must be processed in a different way, into different fragments with multiple relevance for each fragment. A crucial role is also the coherence of a text. Different applications of text summarizations are:

\begin{itemize}
	\item Multi-Document Summarizations
	\item Web Page Summarization
	\item Reports or Meetings
	\item Opinion Summarizations
	\item Scientific Research Papers
	\item News Headlines
\end{itemize}

As an illustration and example for my entire thesis, besides of my own prototype (Chapter \ref{ch:proto}), I start with a case study for the entire Chapter \ref{ch:sota}. 

\textbf{Google News Headline Summarization}

Google has its own news section on this link \textit{https://news.google.com}. Google News automatically generates the news headlines for multi language news articles \cite{google}. Google proposes that for people to digest the large amount of daily information better, they created the long-term goal at the \textit{Google Brain} department to summarize news articles and their headlines as good as possible. The search engine Google is known for its accurate any many search results when using this engine. Google does the same for Google News. Google scrapes news articles all over the world, automatically summarizes it and out of that, it generates the headline of the summarized news article out of it \cite{google}. 
The achieve their state of the art results, Google makes use of a Deep Learning technique called \textit{sequence-to-sequence} learning, which will be explained in Section \ref{ss:seq2seq}. Table \ref{tab:google} shows an example for generated headlines of a summarization. Due to the structure of a news article, for a good result the model who generates the headlines only needs the first few lines of an article.  

All examples from this thesis will be conducted from the perspective of an news article to keep it uniform, whether a long text will be summarized to a shorter text, or only one or two sentences for a headline will be extracted and summarized from an article. 


\begin{table}[]
	\begin{tabular}{|c|c|l}
		\cline{1-2}
		\textbf{Input: Article 1st sentence}                                                                                                                                                                                                                                 & \textbf{Model-written headline}                                                                 &  \\ \cline{1-2}
		\begin{tabular}[c]{@{}c@{}}metro-goldwyn-mayer reported a third-quarter \\ net loss of dlrs 16 million due mainly to the \\ effect of accounting rules adopted this year\end{tabular}                                                                                & \begin{tabular}[c]{@{}c@{}}mgm reports 16 million \\ net loss on higher revenue\end{tabular}    &  \\ \cline{1-2}
		\begin{tabular}[c]{@{}c@{}}starting from july 1, the island province of hainan in \\ southern china will implement strict market \\ access control on all incoming livestock and animal \\ products to prevent the possible spread of epidemic diseases\end{tabular} & \begin{tabular}[c]{@{}c@{}}hainan to curb \\ spread of diseases\end{tabular}                    &  \\ \cline{1-2}
		\begin{tabular}[c]{@{}c@{}}australian wine exports hit a record 52.1 million liters\\  worth 260 million dollars (143 million us) in september, \\ the government statistics office reported on monday\end{tabular}                                                  & \begin{tabular}[c]{@{}c@{}}australian wine exports\\  hit record high in september\end{tabular} &  \\ \cline{1-2}
	\end{tabular}
	\captionof{table}{ The first column shows the first sentence of a news article which is the model input, and the second column shows what headline the model has written \cite{google}.} 
	\label{tab:google}
\end{table}


