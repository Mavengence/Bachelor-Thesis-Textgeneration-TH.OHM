\chapter{An Evolutionary View on the State of the Art}\label{ch:sota}

\epigraph{\textit{Each problem that I solved became a rule, which served afterwards to solve other problems.}}{Rene Descartes}

The goal of this chapter is to survey the development of the text generation from the very beginning until today, 2020. 

As conducted from the Introduction Chapter 1, Text Generation is the generic term for the output part of an automatic text summarizer. The research on Neural Text Generation and other fields had a significant impact on the development of automatic text summarizers. In this chapter, I begin with the definition of a text generator in general and its historical development. I state the most important steps from a generic text generator to a neural text generator. In the following, I focus on the text summarizer and its historical evolution with the impacts of the neural text generators. There are several different human-like summarizing state of the art technologies nowadays for the automatic text summarizers, but they are developed under a large scale data set and demand a powerful graphics unit. Therefore, this chapter is the background knowledge for my prototype, but it also provides a peek into the latest state of the art technology which cannot be applied at home with a low power computer but is only possible to achieve for big companies like Google.


\begin{figure}
	\begin{center}
		\includegraphics[width=6.5in]{photos/used_methods.pdf}\\
		\caption{Tabular overview of methods from the upcoming Chapter 2, which are going to be used in my prototype from Chapter 3. From the Text Generation Concepts, Advanced Approaches for Text Generation and Text Summarization Concepts, I introduce different approaches, but for my prototype, I only use only the text summarization concepts and not the most advanced approaches}\label{used_methods}
	\end{center}
\end{figure}

Table in Figure \ref{used_methods} illustrates the methods which I am going to use in my prototype in Chapter 3. Most of the Sections and Subsections from this current Chapter will find its way into my prototype, except Section \ref{ss:trends}, because it would exceed the limits of my bachelor thesis. The other Sections discuss different approaches through time, but this table shows exactly which specific mentioned approaches I implement in my prototype.


\section{Text Generation Concepts}\label{ss:history}

Text Generation, Language Modeling or Natural Language Generation are different words with similar meanings. I will keep the denotation of text generation throughout this thesis. A widely-cited survey from Reiter and Dale 1997 (Page 57-87) \cite{reiter} characterizes text generation as 'the sub-field of Artificial Intelligence (AI) and computational linguistics that is concerned with the construction of computer systems that can produce readable texts in English or other human languages from some underlying non-linguistic representation of information' \cite{reiter}. This definition rather implies a data-to-text approach instead of the text-to-text approach from Table \ref{tab:nlp_table}, but in 1997 the rule-based approach dominated the neural end-to-end, Neural Text Generation (NTG), methods (Figure \ref{rules_based}). For that reason, in 2003, Evans declares text generation as quite difficult to define \cite{evans} (Page 144-151). Most researchers agree on the text as the output synthesis part of the input-output system (e.g., Text Summarization or Image caption generation \cite{mitchell}), whereas the input part cannot be as easily distinguished \cite{mcdonald} (Page 191-197).

\subsection{Text Generation Tasks}

For the text generation input-output system, the system can be divided into six sub-problems \cite{reiter}. The following bullet points contain the six most crucial steps:

\begin{itemize}
	\item \textbf{Content determination}: Deciding which information to include in the text under construction
	\item \textbf{Text structuring}: Determining in which order information will be presented in the text
	\item \textbf{Sentence aggregation}: Deciding which information to present in individual sentences
	\item \textbf{Lexicalisation}: Finding the right words and phrases to express information
	\item \textbf{Referring expression generation}: Selecting the words and phrases to identify domain objects
	\item \textbf{Linguistic realization}: Combining all words and phrases into well-formed sentences
\end{itemize}

These six tasks illustrate the first approach for early decision processes. 
The chronological order plays a vital role for this tasks. Furthermore, they can be distinct into a strategy and tactics part.
Thompson H. declared these two distinctions for the first time in 1977 \cite{thompson}. Still, when it comes to a modern neural state of the art Text Generation, the steps intersect in some ways. In the following comes a brief introduction to each of the steps. No aggregation is necessary for the headline example.

\subsubsection{Content Determination}\label{ss:content}

The first step is to determine which content should be present in the generated output text. Usually, there is more information stored in the input than in the output. For this reason, a certain \textit{choice} must be undertaken for the content. As mention in the case study (Section \ref{ss:case}, the headline can be summarized very precisely, given only the first few sentences of the news. In this particular case, the determined content could be the first three sentences. 
The process of shortening down a document into a summary, the text's key points must be stored into some collection of preverbal messages.
A semantic representation of information is often expressed in the form of a logical or database like style \cite{gatt}. That can be achieved through grouping semantically similar words and phrases and to remove redundancies (e.g., plays and play into only play). 
This step followed for the most time a rule-based approach, but in recent years researchers developed a data-driven approach (more in Section \ref{ss:archi}). For example, Barzilay and Lee (2004) developed a method to determine the content through Hidden Markov Models (HMM) \cite{lee} (Pages 113-120). Hidden Markow Models are stochastical models named after the Russian mathematician A. A. Markow. They chain up different states of a system, in our case, different topics of one or many news articles. These topics will be automatically clustered together as sentences based on the natural language semantical meaning \cite{gatt}. 

\subsubsection{Text Structuring}

After successfully deciding which contents will be used in the generated text, the structure and order of these fragments need to be determined. Given the example article from Table \ref{tab:google}:

\begin{tcolorbox}
	\begin{center}
		Australian wine exports hit a record 52.1 million liters worth 260 million dollars (143 million us) in September, the government statistics office reported on Monday 
	\end{center}
\end{tcolorbox}

A good news headline should give all the necessary information for the reader, namely:

\begin{itemize}
	\item Where did it happen? -> \textit{Australia}
	\item What happened? -> \textit{Wine exports, record high}
	\item Who did something? -> \textit{Australia}
	\item When did it happen? -> \textit{September}
\end{itemize}

For my example, the content was already predefined in the first step, now the important words and sentences will be reordered based on these four questions. Generalization approaches for the ordering task have already been proposed. Lapatas approach \cite{lapata} (Page 471-484) tries to find an optimal ordering of \textit{information-bearing-items}. This method can even be applied to multi-document input, which is more difficult to solve than single document inputs (explained in Section \ref{ss:multidoc}).

\subsubsection{Sentence Aggregation}

By combining separate sentences with similar meaning into one, the generated text becomes potentially more fluid and enhances the readability \cite{dal} (Pages 383-414) \cite{cheng-mellish2000} (Pages 183-193). For example, an aggregation makes sense for football games and its results published in Google News. Google could web scrape the live tickers of goals, and after collecting all the data, a possible result would be:

\begin{tcolorbox}
	\begin{center}
		(1) Mario Götze scored after 19 minutes and 23 seconds \\
		(2) Mario Götze scored after 20 minutes and 30 seconds \\
		(3) Mario Götze scored after 60 minutes and 11 seconds
	\end{center}
\end{tcolorbox}

The three tickers are not redundant, because they contain new information in every sentence, but for summarizing it, the sentences can be aggregates into:

\begin{tcolorbox}
	\begin{center}
		(4) Mario Götze scored 3 times within 51 minutes
	\end{center}
\end{tcolorbox}

Aggregation is not an easy task because it is not intuitive for an algorithm to detect semantic similarities and, at the same time, new information in that. Furthermore, it depends highly on the to achieving output which kind of aggregation the text should undergo. White and Howcraft (2015) propose a general approach to this problem. They designed an algorithm to detect parallel verb phrases (\textit{scored after}) in multiple (three) sentences and elide the subject and the verb in the generated sentence \cite{white-howcroft-2015-inducing} (Pages 28-37). 

\subsubsection{Lexicalization}\label{ss:lex}

After the sentences have been aggregated and finalized, the next step is lexicalization, which converts the sentences into natural language. A single event can be expressed by natural language in multiple ways. For example, the scoring event from the last section could be expressed as \textit{scored three goals} or \textit{goaled for three times}. The complexity of the lexicalization step correlates with the number of alternative sentences available. Furthermore, it is important if their summary is limited with an amount of variation \cite{Theune} (Pages 47-86). Whether or not the text shall be processed with lexical variation in its generated sentences or not depends on the application field. This process decision needs to be decided in advance. For example, the soccer game is more likely to be converted into different styles than a weather forecast. Another important difficulty is to design the way in how the lexicalization cares about gradable properties. If the live ticker was:

\begin{tcolorbox}
	\begin{center}
		(1) Mario Götze scored fast after 3 minutes and 23 seconds
	\end{center}
\end{tcolorbox}

Then the system needs to know whether the football player scored fast in a way that it is an early stage of the game, or he ran in such a fast way and scored with the pace. Humans tend to perceive meanings of words differently, as Power and Williams (2014) pointed out in an evaluation. The time \textit{00:00} can be perceived as \textit{midnight}, \textit{late evening} or simply even \textit{evening} for some people \cite{Power} (Pages 113-134).


\subsubsection{Referring Expression}\label{ss:ref}

Referring Expression Generation is highly characterized by Dale and Reiter in 1997. They came up with the idea to identify words and phrases as domain entities. Nowadays, this is also known as \textit{Named Entity Recognition}. This step shows some similarity to lexicalization, but Dale and Reiter pointed out that expression referring is \textit{a discrimination task, where the system needs to communicate sufficient information to distinguish one domain entity from other domain entities} \cite{reiter2}. From the previous example, \textit{Mario Götze} can be denoted with his name. Another way would be calling him \textit{football professional} or \textit{the athlete}. Many factors play a role in how to determine which expressions and factors play a role in a particular context. Referring expression generation can be broken down into two steps.
The first step is to decide the shape of a referring expression. What type of reference should be used (e.g., a proper name or described with his/her job) \cite{named}. The second is to determine the content of the referring expression (e.g., Mario Götze or the athlete) \cite{named}.
Rule-based approaches, as well as the state of the art Machine Learning approaches, have been proposed to solve this task \cite{reiter2}.  

The usual limitation of previous referring expression generation systems is that they are not able to generate referring expressions for new, unseen entities \cite{anja} (Pages 294-327). With the use of modern Machine Learning approaches, this limitation has overcome.
Many tools, for example, the \textit{Natural Language Processing Toolkit NLTK}, allow the easy transformation from the lexicalized sentence into its named entities. The sentence \textit{Mario Götze scored fast after 3 minutes and 23 seconds} will be transformed into:

\begin{tcolorbox}\label{box:1}
	\begin{flushleft}
		(1) [('Mario', 'NNP'), \\
		(2) ('Götze', 'NNP'), \\
		(3) ('scored', 'VBD'),\\
		(4) ('fast', 'RB'),\\
		(5) ('after', 'IN'),\\
		(6) ('3', 'CD'),\\
		(7) ('minutes', 'NNS'), \\
		(8) ('and', 'CC'),\\
		(9) ('23', 'CD'),\\
		(10) (Seconds', 'NNS')]
	\end{flushleft}
	
	\begin{itemize}
		\item \textbf{NNP} = noun, proper, singular (1,2)
		\item \textbf{VBD} = verb, past tense (3)
		\item \textbf{RB} = adverb (4)
		\item \textbf{IN} = preposition or conjunction (5)
		\item \textbf{CD} = numeral, cardinal (6, 9)
		\item \textbf{NNS} = noun, common, plural (7, 10)
		\item \textbf{CC} = conjunction, coordinating (8)
	\end{itemize}
\end{tcolorbox}

Assigning words to named entities can even be used on its on to detect organizations, people or other relevant objects in a sentence.

\subsubsection{Linguistic Realization}

Finally, after detecting all relevant words and structures of the input text, it only needs to be combined into a well-formed sentence. Usually referred to as linguistic realization, this task involves ordering constituents of a sentence, as well as generating the correct morphological shapes (e.g., verb conjugations) \cite{gatt} (Pages 18-20). The special task in this step is whether the generated output needs to make use of words not present in the given text. In the case of text summarization, this is often referred to as an \textit{abstractive} or \textit{extractive} approach (shown in Section \ref{ss:exabs}). This task can be thought of a non-isomorphic (not reservable, because the same word can have different named entities dependent on the sentence) structure \cite{ballesteros-etal-2015-data} (Pages 387-397). The three most common approaches for the realization are: \\

\begin{itemize}
	\item Human-crafted templates
	\item Human-crafted grammar-based systems
	\item Statistical approaches
\end{itemize}

The most modern and widely used way is the statistical approach, but within that, there are several different methods to make use of the statistics. To give an example, Bohnet et al. (2010) \cite{bohnet-etal-2010-broad} (Pages 98-106) describe a realizer with underspecified dependency structures as input, Support Vector Machine (SVM) based environment. The classifiers are organized in a cascade to decode semantic input into the corresponding syntactic features. A Support Vector Machine is an algorithm to classify input, whether it belongs to a specific topic or e.g., named entity from the last section or not. SVM is a machine learning approach, and it tested on a standard metric for measuring the accuracy of a generated e.g., text summarization called BLEU. This metric will be explained in Section \ref{ss:ev}.
Furthermore, the more decision the statistical generating system makes, and the more complicated it becomes, the more abstract the output will be \cite{gall} (Page 21). This paves the way for a stochastical end-to-end system like Konstas and Lapata showed in 2013 \cite{Konstas:Lapata:2013}. They present a first step towards the automated text summarizations.

\subsection{Architectures and Approaches}\label{ss:archi}

After the overview of the six main tasks for Text Generation systems, this section focuses on the way those tasks are organized together. There are three main approaches for the Text Generation architecture shown in the dark boxes in Figure \ref{architecture}. The light boxes illustrate the outputs from the main stages.

\begin{figure}
	\begin{center}
		\includegraphics[width=6in]{photos/architecture}\\
		\caption{Classical 3-stage Text Generation architecture, after Reiter and Dale (2000) \cite{reiter2} }\label{architecture}
	\end{center}
\end{figure}

Since the design of the modules in Figure \ref{architecture}, a lot has changed. The modular view is challenged by the planning-based and data-driven approach, because the modular view is not flexible enough for modern requirements. Still, it provides a good sequence structure, and the original idea remains until now. The following three approaches will are explained in more detail in this section:

\begin{itemize}
	\item Rule-based, modular approaches
	\item Planning-based approaches
	\item Data-driven approaches
\end{itemize}

\subsubsection{Rule-based approach}\label{ss:rule}

The rule-based or modular approach shown in Figure \ref{architecture}, is a classical approach from the early Artificial Intelligence research. It is designed to show a clear division between the sub-tasks, but with sometimes big variations among them. The three-stage architecture was originally called \textit{consensius pipeline} because it is in the design of a pipeline, and it was the de-facto standard in the year 2000 \cite{reiter2}. This pipeline share many similarities with the state of the art architecture used for text summarization in the year 2001, introduced by Mani et al. \cite{mani}. It can be broken down into the following steps \cite{gatt} (Page 23):

\begin{itemize}
	\item \textbf{Analysis} of the source text (single or multi-document). This first stage includes \textit{Text Planning}, which shares similar aspects with the Text Planner from Figure \ref{architecture}. One of the tasks for this step is \textit{Content Determination}.
	\item \textbf{Transformation} of the selected input. This phase includes processing steps like \textit{Text Structuring} and \textit{Text Aggregation} on the selected text. It is especially important when it comes to abstractive text summarization (Section \ref{ss:exabs}). This phase shares several similarities to the \textit{Sentence Planner} from Figure \ref{architecture}.
	\item \textbf{Synthesis} produces the summary of the input based on the transformed selected input. The higher the abstraction level of the output should be, the more important this phase becomes. It can be seen as the \textit{Realizer} with the methods of the \textit{Linguistic Realiser} from the previous section.
\end{itemize}

The strict breakdown into clear stages (modules) comes with the cost of decreased flexibility. There is not always a rule for each task, but the abstractive based methods for Text Generation achieve the state of the art results. Those alternative approaches with a better abstraction level come on the other hand with the cost of blurred boundaries in the single stages. The basic idea is to create hand-crafted templates for all possible circumstances. To keep up with the football example, a template could look like this:

\begin{tcolorbox}
	goals(
	\begin{addmargin}[1cm]{1cm}
		player-name = 'Mario Götze',\\
		minute = 19,\\
		seconds = 23,\\
		player-number = 10
	\end{addmargin}
	
	)\\
	
	foul(
	\begin{addmargin}[1cm]{1cm}
		by-player = 'Mario Götze',\\
		to-player = 'Thomas Müller',\\
		minute = '20',\\
		second = '12'
	\end{addmargin}
	)
\end{tcolorbox}

Even for a single football game, it is not possible to create a template with all possibilities. There may be some cases in which the possible combination of semantic units is not as big as in a single football game. In these cases, a rule-based template could make sense, but for the most modern use-cases, this approach is obsolete.

Even in rule-based architectures, there have been many developments proposed, but for the sake of simplicity, I want to discuss the modern approaches in more detail than the old ones.

\subsubsection{Planning-based approach}

In the Artificial Intelligence field, the planning problem can be described as the process of detecting a sequence of one or more actions to satisfy the goal to be achieved \cite{gatt} (Page 25). The classical planning-based approached was introduced by Fikes and Nilsson back in 1971 \cite{Fikes:71} (Pages 189-208). The idea was to store actions into tuples containing the preconditions and effects of the action, respectively. In this way, planning-based means to regard language as an action \cite{clark}. 

Basically, no restriction prevents the actions from choosing a type that can be inserted into a plan, plan-based approaches cut across the edges of many natural text generation tasks that are normally strictly stacked in the classical pipeline architecture (Figure \ref{architecture}) \cite{gatt}. This means that the plan-based approach does not rely on the pipeline architecture, but steps are whirled together and follow different sequences than usual. This allows the input to be more flexibly processed. The most modern way for a planning-approach is the \textit{stochastical planning using Reinforcement Learning}. Reinforcement Learning means that the algorithm has an implemented reward and punishment variable, which allows the algorithm to notice by itself when a certain action will be rewarded or punished. The reward and punishment need to be manually configured. The text generation process could be modeled by a Long Short Term Memory (LSTM) (explained in Section \ref{ss:lstm}. The time transitions \textit{t} and the following \textit{t+1}, ... , are associated with a reinforcement signal, via the reward or punishment function to adjust the behavior of the desired output. 

Rieser et al. (2011) pointed out that this approach is useful in optimizing information presentation when generating restaurant recommendations \cite{rieser}. Janarthanam and Lemon (2014) applied this method to improve the choice of information for selecting a referring expression, given the knowledge of the user.  As the user acquires new knowledge in the course of a dialogue, the system learned to adapt its behavior by changing its internal user-model \cite{janarthanam-lemon-2014-adaptive} (Pages 883-920).

\subsubsection{Data-driven approach}\label{ss:data}

Data-driven models experience more and more attention recently in the text generation community. They provide the flexibility and potential to overcome the template-based approaches. Even in the past six years, there have been plenty of studies that show the successes of data-driven models. For example, Dinu and Baroni (2014) stated out that text generation can be performed on different distributional semantic models \cite{dinu-baroni-2014-make} (Pages 624–633). Another example is from Swanson in 2014 as well, where he performed text generation with language modeling on a specified vocabulary constraint \cite{swanson-etal-2014-natural} (Page 124). All of these methods and data-driven approaches, in general, require much training data. The first step for such a system is to build up a so-called \textit{corpus}. A corpus can be viewed as a kind of template, but the way it is used is entirely different than the template from the rule-based approach. The corpus is created through the training of a vast training data set. It consists of a basic set of utterances, and it can be further manually extended and redefined \cite{elena} (Page 4). The data-driven model can even be used for a general-purpose machine translation system, which can respectively even be adapted to a specific domain itself (shown by Wang et al. \cite{wang-hirst-2009-extracting} Pages 471–477). The corpus now contains a set of vocabulary that can be extended through adding new training instances (e.g., user inputs or synonyms) into the \textit{lexicon}. 
Extending and diversifying the corpus enhances the quality of the interaction between the system and the user and further enriches the conversational level, but mostly the system's response. This can be regarded as a higher abstraction level, used for the modern text summarization systems. 

It can be seen in Figure \ref{corpus}, that there is a total amount of 34283 words in the English part of the TownInfo corpus and that there are only 462 distinct tokens. This includes even various units as proper names and numerals. The example shows an approach to translate from English into French and vice versa. All of the \textit{Advanced Approaches for Text Summarization} (Section \ref{ss:trends}) are fully data-driven. 

The prototype from Chapter 3 is based on a data-driven approach as well because my text summarizer makes use of Long Short Term Memory cells. More specifically, it is an enhanced modification, namely an Attention model. This data-driven approach will be explained in the next Section \textit{Advanced Approaches for Text Generation}.

\begin{figure}
	\begin{center}
		\includegraphics[width=3in]{photos/corpus}\\
		\caption{Example Corpus from \cite{elena} Page 103}\label{corpus}
	\end{center}
\end{figure}

\section{Advanced Approaches for Text Generation}\label{ss:aatg}
\subsection{Recurrent Neural Networks}\label{ss:rnn}
Even though I introduced the neuron in a neural network as a kind of brain cell imitation, a neuron of a basic neural network cannot process time-depending input like language, unlike the brain. Making information persistent is a crucial step towards better performing models. Recurrent neural networks, or RNN, address this issue. They are networks with integrated loops, which allow the information to persist over time and input new words a sentence into future time steps into the network\cite{olah}. The network architecture of the RNN is important, because it denotes the first step into neural text generation and neural text summarization. 

\begin{figure}
	\begin{center}
		\includegraphics[width=4.5in]{photos/RNN-unrolled}\\
		\caption{Recurrent Neural Network with integrated loops \cite{olah}}\label{rnn}
	\end{center}
\end{figure}

Figure \ref{rnn} shows an unrolled Recurrent Neural Network. The input \(x_t\) on time step \textit{t}, is passed to the neural network \textit{A}. The network looks at the input on this time step and outputs the hidden state \(h_t\) at the same time step \textit{t}. This loop allows the network to pass information from one time step to another. The picture \ref{rnn} shows, that the learned parameter from input \textins{x} on time step \textins{t} will be passed as additional information to the next time step \textins{t + 1} and so on. For example if a RNN wants to predict the next word in the sentence "Since I am living in Hong Kong .. by now I speak fluent \textit{Cantonese}". The network needs to remember that the target country is Hong Kong to predict the language Cantonese. At each time step \textit{t}, the hidden state \textit{\(h_t\)} of the Recurrent Neural Network is updated by:

\begin{tcolorbox}
	\begin{center}
		\begin{math}
		\boldsymbol{h_{(t)}} = f(\boldsymbol{h_{(t-1)}}, x_{t})
		\end{math}
	\end{center}
\end{tcolorbox}

\begin{tcolorbox}
	\textbf{Where} \\
	Time step = \textit{t} \\
	Input =     \textit{x} \\
	Hidden state = \textit{h} 
\end{tcolorbox}


where \(f\) is a non-linear activation function and \textit{x} is the input in form of a word. The function \(f\) can be in the simplest case a sigmoid function which has either 0 or 1 as output, or the more complex and effective Long Short Term Memory cell, explained in the next Section \ref{ss:lstm} \cite{hochreiter1997long}. The Recurrent Neural Network is trained to predict for example the next word in a sentence or sequence. This prediction is possible due to the learned probability distribution over a sequence. The output at each time step \textit{t} is a conditional distribution

\begin{tcolorbox}
	\begin{center}
	 \(p(x_{t}|x_{t-1},...,x_{1})\)
	\end{center}
\end{tcolorbox}
Where \(x_{t}\) (\textit{x} on time step \textit{t}) is dependent on all previous \textit{x} from time step 1 until \textit{t-1}.
Theoretically, with this approach, it is possible to retain information from many time steps ago, but unfortunately, as the time-span back grows, RNN's become unable to learn the information from too long ago cells. This phenomenon was explained by Sepp Hochreiter in 1991 \cite{Hochreiter:91} under the name \textit{vanishing gradient problem}. The solution to this problem is the Long Short Term Memory, short LSTM.

\subsection{Long Short Term Memory}\label{ss:lstm}
Long Short Term Memory cells were first proposed by Sepp Hochreiter and Jürgen Schmidhuber in 1997 \cite{hochreiter1997long}. The LSTM is a special kind of Recurrent Neural Network, because it can remember long-term dependencies and information. The goal of the cell is to solve the vanishing gradient problem of the Recurrent Neural Network. Inputs into this cell can be stored for an extended period, without forgetting them, as in Recurrent Neural Networks. The LSTM is designed to avoid the loss of information (vanishing gradient problem), by intentionally passing on specific information over plenty of time steps. 

\begin{figure}
	\begin{center}
		\includegraphics[width=4.5in]{photos/LSTM3-SimpleRNN}\\
		\caption{The repeating module in an Recurrent Neural Network contains one single layer \cite{olah}}\label{lstm}
	\end{center}
\end{figure}

LSTM's can be enrolled the same way as RNN's, but there is a core difference between the Recurrent Neural Network in Figure \ref{lstm} and the Long Short Term Memory in Figure \ref{lstm2}. 

Figure \ref{lstm} shows three cells of a Recurrent Neural Network, where each cell receives the input \textit{x} at the time steps \textit{t-1} (past), \textit{t} (present) and \textit{t+1} (future).  Looking at a specific cell, the predecessors cell output is squeezed together with the presents cells input into the \textbf{tanh} function to create a so called \textit{activation}. Depending on the two inputs for the \textbf{tanh} function, the output of the \textbf{tanh} function gets either triggert (\textit{activated}) or not and is saved into the hidden cell at the same time step as the input.

This is a rather simple architecture compared to the Long Short Term Memory cell in Figure \ref{lstm2}. There is one additional input and output in each LSTM cell, but a lot more additional mathematical operations than in the RNN cell.

The LSTM has four gates instead of one like the RNN. The four gates are:

\begin{itemize}        
	\item Forget Gate
	\item Input Gate
	\item Cell State
	\item Output Gate
\end{itemize}

\begin{figure}
	\begin{center}
		\includegraphics[width=4.5in]{photos/LSTM3-C-line}\\
		\caption{Cell State of the Long Short Term Memory which acts as data highway \cite{olah}}\label{lstm3}
	\end{center}
\end{figure}

The \textbf{Forget Gate} decides what information should be thrown away or kept. Information from the previous hidden state and information from the current input is passed through a sigmoid function. A sigmoid function takes an input and returns high values closer to 1 and smaller values closer to 0. The closer to 0 means to forget the state, and the closer to 1 means to keep the state.

The \textbf{Input Gate} updates the cell state. That decides which values will be updated by computing the values to be between 0 and 1 like the Forget Gate. Important information is closer to 1 and 0 means less important.

The \textbf{Cell State} is the core of the LSTM. It is the horizontal line shown in Figure \ref{lstm3}. The cell state acts like the information highway in the network. With only some minor linear computation, it runs through the entire cell. This way, information can pass very easily through the entire network. 

The \textbf{Output Gate} decides what the hidden state of the next LSTM cell should be. The hidden state contains information on previous inputs, and it is also used for predictions. The hidden state denotes the state which is passed from the output gate on time step \textit{t} to the input gate for the LSTM cell on time step \textit{t+1}.


\begin{figure}
	\begin{center}
		\includegraphics[width=4.5in]{photos/LSTM3-chain}\\
		\caption{The repeating module in an LSTM contains four interacting layers \cite{olah}}\label{lstm2}
	\end{center}
\end{figure}

The main idea of the LSTM is that it can decide which information to remove, to forget, which to store and when to use it. It can also decide when to move the previous state information to the next, like the RNN shown in Figure \ref{rnn}. Even though many variations of the LSTM occupy the state of the art performance, the LSTM is used in many real business cases in production, like the Google translater or weather forecasting.
The Long Short Term Memory paved the way for the sequence to sequence models.

\subsection{Sequence to Sequence}\label{ss:seq2seq}
In the year 2014, Google invented a new way to translate language by learning a statistical model with a neural machine translation approach \cite{seq2seq}. Google called it Sequence to Sequence model \cite{seq2seq}, often shortened down to seq2seq, which consists of an encoder and a decoder. 

Before that, language translation was originally processed by rule-based systems \cite{chen-goodman}. The systems computed their work by breaking down sentences into plenty of chunks and translating them phrase-by-phrase, but this approach created not easily understandable language.

After rule-based systems, statistical models have taken over the. Given a source text in e.g. German (\textit{f}), what is the most suitable translation into e.g. English (\textit{e})? The statistical model p(\textit{g}|\textit{e}) is trained on multiple texts (corpus) and finally outputs p(\textit{e}), which is calculated only on the target corpus in English. 

\begin{tcolorbox}
	\begin{center}
		\begin{math}
		\hat{e} = argmax_{e}(e | g) = argmax_{e} p(g | e) p(e)
		\label{eq:rule}
		\end{math}
	\end{center}
\end{tcolorbox}

\begin{tcolorbox}
	\textbf{Where} \\
	\textit{g} = a german source text \\
	\textit{e} = the english desired output text \\
	\textit{p(e|g)} = the conditional distribution \\
	\textit{argmax} = selecting the result with the highest probability
\end{tcolorbox}

The formula means, among all Baysian probabilities p(\textit{g} | \textit{e}) p(\textit{e}), select the pair of words (translation), select the most likely to be the best translation (argmax). Even though this approach produces good results, it looses the wider semantical view, and so it is especially not effective for a good summarization technique.

For the first time, neural networks in the form of feed-forward fully-connected neural networks produced such excellent results, that they replaced all non-network techniques. Affine matrix transformations are stacked together and are followed by non-linearities to the input and each following hidden layer \cite{Bengio} Page 1141-1142. However, these models require a fixed content length for their calculations, which makes them again not flexible enough to produce human-like translations.  

Even if a LSTM (Section \ref{ss:lstm}) was used to map sequences of words from one language to another, it will most likely produce errors or bad results. A single LSTM cell needs the same input length and output length, which is unrealistic for translating multilingual. For example

\begin{tcolorbox}
	\textit{g} = Er rennt \\
	\textit{e} = He is running 
\end{tcolorbox}

if the source \textit{g} is translated into English \textit{e}, the length of the source and output is different.
The LSTM itself can not translate that, because of the different word length. 
The Long Short Term Memory cell from Section \ref{ss:lstm} was invented independently from the sequence to sequence models, but
finally, three employees of Google published a paper about their approach to making use of the LSTM to create a sequence to sequence model, also called the encoder-decoder model.
The basic idea is that the encoder converts an input text to a latent vector of length \textit{N}, and the decoder generates an output vector of length \textit{V} by using the latent encoded vector. It is called a latent vector because it is not accessible during the training time (manipulating it). For example, in a standard Feed Forward Neural Network, the output of a hidden layer in the network can not be manipulated. The first use of encoder-decoder models was for machine translation.

Technologies for a specific field in the machine learning environment and especially text generation can often be used cross-functional. The encoder-decoder model found its way into text summarization and automated email reply by Google \cite{google} as well. Figure \ref{enc-dec} illustrates the model for Google's automated email reply.

\begin{figure}
	\begin{center}
		\includegraphics[width=5.5in]{photos/encoder_decoder}\\
		\caption{LSTM encoder-decoder model for automated E-Mail reply}\label{enc-dec}
	\end{center}
\end{figure}

Figure \ref{enc-dec} makes use of a Long Short Term Memory cell, which captures situations, writing styles and tones. The network generalizes more flexible and accurate than a rule-based model ever could \cite{google}. 


\subsection{Encoder and Decoder}\label{ss:encdec}

In the following, the encoder and then the decoder will be explained to have a better insight into how these models work. My prototype from Chapter \ref{ch:proto} is based on this kind of model. As already mentioned, a sequence to sequence model is often referred to as an encoder-decoder model. The sequence to sequence model itself is built using a Recurrent Neural Network or a Long Short Term Memory, as explained in the last Section \ref{ss:seq2seq}.

\begin{figure}
	\begin{center}
		\includegraphics[width=4.5in]{photos/encoderdecoder.jpeg}\\
		\caption{Encoder-decoder sequence to sequence model \cite{encdec}}\label{encdecseq}
	\end{center}
\end{figure}

Figure \ref{encdecseq} shows that the encoder-decoder structure consists of three parts:

\begin{itemize}
	\item Encoder
	\item Intermediate (encoder) Vector
	\item Decoder
\end{itemize}


The \textbf{Encoder} iteratively integrates the words from a sentence into the hidden state \textit{h} and further into the Long Short Term Memory cell.
Figure \ref{lstm3} shows a single LSTM cell with the input cell state \textit{C} at the time step \textit{t-1} and the input of the hidden state \textit{h} at the same time step \textit{t-1}. The cell must compute both the input words, but also the knowledge from the prior words. Words are represented as latent vectors in the sequence to sequence models and are stored in a vocabulary table. Each fixed-length vector stands for a word in the vocabulary; for example, the vector length is fixed to a dimension of 300. In a simple case, the number of words in the vocabulary is fixed to e.g., 50.000 words, hence the dimension of the vocabulary table in Figure \ref{voctable} is [50000 x 300]. 

\begin{figure}
	\begin{center}
		\includegraphics[width=3.5in]{photos/w1-16}\\
		\caption{Snippet of an example vocabulary table. This table shows actually 50000 words where aardvark is the first and zoology is at position 49999. To represent the words in a vectorspace (meaningspace), I use the artificially chosen dimension size of 300. For this reason the table shown has a size of [50000, 300]\cite{mugan}}\label{voctable}
	\end{center}
\end{figure}

A connection of multiple recurrent units (three in Figure \ref{encdecseq}) where each accepts a single element as an input, gains information and propagates it forward to the next cell and accordingly the next time step. In the example of Figure \ref{encdecseq}, the hidden state of \textit{h3} is calculated based on the prior two cells.

\begin{figure}
	\begin{center}
		\includegraphics[width=3.5in]{photos/meaningspace}\\
		\caption{Meaningspace of neural text generation \cite{mugan}}\label{meaningspace}
	\end{center}
\end{figure}

The \textbf{Encoder Vector} is the last hidden state of all the encoder cells; in this example, the encoder vector is located at the output of cell three. The vector tries to combine all of the information from the prior encoded-words with the purpose to help the decoder make accurate predictions. The encoder vector is the initial input for the decoder part of the model. \\ \\
The \textbf{Decoder} unrolls the encoder vector from meaning space into a target sentence. The meaning space (shown in Figure \ref{meaningspace}) is a mapping of concepts and ideas that we may want to express to points in a continuous, high-dimensional grid \cite{mugan}. The world state in this figure represents a sentence like in the box below \textit{"Since I am living in Hong Kong, by now, I speak fluently..."}. The encoder is a neural network that maps this sentence into a vector. This is necessary, because the computer doesn't under words, but only numbers. For this reason the words must be transformed into meaningful number representation as shown in Figure \ref{voctable}.
The minimum requirement for the meaning space is to consist at least of the last state of the encoder Recurrent Neural Network (encoder vector). The decoder computes a probability distribution for each word in the encoder vector to generate the next state. 

\begin{tcolorbox}
	Example encoder vector of the word \textit{Cantonese}: \\
	
	[2.34, 2.15, 3.45, 2.32, ..., 1.36] \\
	
	where this vector has the shape [1 x 300]
\end{tcolorbox}

In the example case, the output is generated by multiplying the hidden state in the encoder vector \textit{h} by the output matrix of size [300 x 50000]. The product of this matrix multiplication is a vector of size [50,000], that can be normalized with a \textit{softmax} into a probability distribution over words in the vocabulary. The network can then choose the word with the highest probability because the softmax squeezes all outputs into a summed up probability of 1. For example:

\begin{tcolorbox}
	"Since I am living in Hong Kong, by now, I speak fluently... "
	
	\begin{itemize}
		\item Cat: 0.01
		\item running: 0.005
		\item Cantonese: \textbf{0.5}
		\item Mandarin: 0.3
		\item French: 0.015
	\end{itemize}
	
	The chosen word is \textbf{Cantonese} because it has the highest probability among all probabilities which are summed up to 100\%
\end{tcolorbox}
 

\subsection{Attention}\label{ss:atten}
In general, the explanation of the sequence to sequence models just covered the fundamental idea of the model. To achieve the state-of-the-art result, not only a single vector can be used for encoding the entire input sequence, but also multiple vectors where each is capable of capturing other information from the input. 

\begin{figure}
	\begin{center}
		\includegraphics[width=6.5in]{photos/attenion}\\
		\caption{Baseline sequence-to-sequence model with attention \cite{see-etal-2017-get} }\label{attention1}
	\end{center}
\end{figure}

In the encoder and decoder model, the length of the state vector \textit{h} does not change for the input and output. As shown in the example of Section \ref{ss:seq2seq}, sentences translated into another language can have a different word length. For the model to automatically adjust the length of the output, is to use the technology called \textit{attention} \cite{attention} \cite{attention2}.

Figure \ref{attention1} shows the basic concept of attention for the summarization task of a news article.  The attention model tries to identify relevant words and meanings in the source text to generate new words. This means generating words that are not included in the input document, for example, using the word \textit{to beat} in the abstractive summary instead of won \cite{see-etal-2017-get}.

Attention enables the network to look at all prior encoded states of the words, takes the weighted average probability of the vectors, and also uses this as additional information. Sequence to sequence models can be entirely built up from the attention model \cite{attention2}. 

\section{Text Summarization Concepts}

In the modern era of big data, retrieving useful information from a large number of textual documents is a challenging task due to the unprecedented growth in the availability of online blogs, forums, news, and scientific reports that are tremendous. Automatic text summarization provides an effective and convenient solution for reducing the amount of time it takes to read all the information. The goal of text summarization is to compress long documents into shorter summaries while maintaining the essential information and semantic of the documents \cite{ts-intro} \cite{ts-intro2}. Having the short summaries, the text content can be retrieved, processed and digested effectively and efficiently. 
Generally speaking, there are two basic approaches for performing a text summarization: Extractive and Abstractive \cite{ts-intro3}. 

As mention from the Section \ref{ss:history}, there is a text-to-text and data-to-text approach. The text-to-text method is mostly used in the context where a single document is the only input for generating the text. It can be seen as an extractive approach.  On the other side, the abstractive approach is a data-to-text method, because it takes multiple inputs into account, such as the text, opinion or another vocabulary.

Jones et al. in 1999, defined and classified text summarization by the following three summarization factors \cite{Jones98automaticsummarising:} (Pages 1-12):

\begin{itemize}
	\item \textbf{Input}: single and multi document
	\item \textbf{Purpose}: informative and indicative
	\item \textbf{Output}: extractive and abstractive
\end{itemize}

These three factors are explained in the following.

\begin{figure}
	\begin{center}
		\includegraphics[width=5.5in]{photos/history}\\
		\caption{Highlights of automatic text summarization \cite{juan} (Page 17)}\label{hist}
	\end{center}
\end{figure}

Figure \ref{hist} provides a broad overview of the development of automatic text summarization. Everything started with Peter Luhn from Germany in 1958.

\subsection{Input}\label{ss:input}

The input has two crucial variables that can change the way how to process the summary completely. The input variable denotes whether the input comes from a \textbf{single-document} or from a \textbf{multi-document}. 

\subsubsection{Single-Document}\label{ss:sd}
The most simple task for an automized summarization system is a generic single document summary. Peter Luhn's work first introduced that system in 1958, an extractive method to summarize a text \cite{textmining1958}.  Even though this first approach was proposed over 60 years ago, it is still up to date. The pipeline for a single document summary is as follows \cite{juan} (Page 69):

\begin{itemize}
	\item \textit{Preprocessing}: Split sentences into words, cut stopwords (and, that, there, ..) out and filter out punctuation 
	\item \textit{Extraction}: Calculate and combine similarity measure between words and/or sentences, sort and select the sentences
	\item \textit{Generarion}: Assemble, postprocess and reformulate extracted sentences
\end{itemize}

\begin{figure}
	\begin{center}
		\includegraphics[width=4.5in]{photos/single_doc}\\
		\caption{General architecture of a extraction-based single-document summarization system \cite{juan} (Page 70)}\label{single}
	\end{center}
\end{figure}

Figure \ref{single} shows a standardized pipeline, also called \textit{Natural Language Processing Pipeline}. For measuring the similarity between words and sentences, several methods are possible to achieve this goal. It would exceed this thesis if I explain all different methods, but I still want to mention them, because it is the core part of the original pipeline middle-step:

\begin{itemize}
	\item Latent Semantic Analysis (LSA)
	\item Graph-based approaches
	\item Statistical metrics
\end{itemize}

The \textbf{Latent Semantic Analysis} \cite{Deerwester90indexingby} is a model that allows semantics to be represented from the following ideas: two words are semantically close if they appear in similar contexts and two contexts are similar if they contain semantically close words. The words in a (large) corpus are represented in the occurrence matrix S. The matrix S stores, for every single word in the corpus, the contexts in which the words appeared and additionally also their appearance frequency \cite{juan} (Page 73). Therefore, this technique measure relationships between different words. After finishing this process, the Latent Semantic Analysis assumes accordingly that words close in meaning occur in similar pieces of text.

\begin{tcolorbox}
	\textit{S} = Matrix
\end{tcolorbox}

\textbf{Graph-based approaches} conduct to represent the content of textual information from single documents. There are countless variations of graph-based approaches, and I have already used one so far. In Section \ref{ss:ref} for the Box \ref{box:1}, the part-of-the-speech tagging is a method in the graph-based approaches. In general, the vertices or nodes are assimilated to semantic collections of words and sentences, and the edges of the nodes represent the relations between each word and collections. 
Another widely used approach is \textit{bag-of-word}. I used that as an example in Section \ref{ss:data} for Figure \ref{corpus}. Different unique words are packed together into a corpus, and each occurrence of the word is counted and summed together. The ANK algorithm from Lawrence Page in 1998 \cite{brin1998anatomy} (Pages 107-118) paved the way for the success in web page retrieval (scraping): web pages are ranked by their popularity in the network (how often each page is clicked by users), rather than by the amount or quality of their content. This type of algorithm computes the importance of the vertex of the graph, based on the general information gathered from a recursive analysis of the complete graph, rather than a local analysis of a vertex \cite{juan} (Page 77).

\subsubsection{Multi-Document}\label{ss:multi}

Multi-document summarization faces different problems than the single-document summarization. The sentence extraction methods (Latent Semantic Analysis and graph-based approaches) can also be applied to multi-document summarization. The problem of redundancy in the documents can always be present in multi-document summarization. Typically this does not happen in the single-document case. Redundancy has a significant impact on the coherence and the cohesion of this new type of extract \cite{juan} (Page 109). Multi-document summarization is the extension of single document summarization, but like already said, redundancy is not the only issue. The primary pipeline is shown in Figure \ref{multi}. 

\begin{figure}
	\begin{center}
		\includegraphics[width=4.5in]{photos/multidoc}\\
		\caption{Extraction based multidocument summarization system \cite{juan} (Page 110)}\label{multi}
	\end{center}
\end{figure}

Multi-document input is likely to have the same or a similar topic, but it is not necessary. The first automation system was developed by McKeown and Radev in 1995 until 1998 \cite{radev-mckeown-1998-generating} (Pages 469-500). My case study example Google News is a typical example for multi-document summarization, because Google uses multiple sources of information (scrape news from different other websites), collect the information and process them through the pipeline (Figure \ref{multi}). The key task is not only detecting and eliminating redundancy but also notice novelty and ensure that the generated summary is coherent and without missing points, \cite{Das07asurvey} (Page 11).
 
The input for the summarization system are multiple documents such as 

\begin{tcolorbox}
	\begin{center}
		\(D_{1}, D_{2},..., D_{n}\),
	\end{center}
\end{tcolorbox}

\begin{tcolorbox}
	\textbf{Where} \\
	\textit{D} = single document \\ 
	\textit{n} = the total number of single-documents
\end{tcolorbox}

combined into a multi-document input.
The preprocessing step in the multi-document summarization is quite similar, but the same as in the single-document summarization. The four steps for the multi-document are:
 
 
\begin{itemize}
	\item \textbf{Sentence segmentation}: Each document \textit{D} is segmented for itself as 
	\begin{tcolorbox}
		\begin{center}
		\(D = {S_{1}, S_{2},..., S_{m}}\)
		\end{center}
	\end{tcolorbox}
	\begin{tcolorbox}
		\textbf{Where}\\
		\textit{D} = a single document \\
		\textit{S} = single sentence in a document \textit{D}\\
		\textit{m} = number of sentences \textit{S} in document \textit{D}
	\end{tcolorbox}
	where every \textit{S} denotes a single sentence in document \textit{D}. The number of sentences is denoted as \textit{m}.
	\item \textbf{Tokenization}: Words of each sentence \textit{S} are tokenized into 
	\begin{tcolorbox}
		\begin{center}
			\(T = {t_{1}, t_{2},..., t_{k}}\) 
		\end{center}
	\end{tcolorbox}
	\begin{tcolorbox}
		\textbf{Where} \\
		\textit{t} = a single token in the sentence \textit{S} \\
		\textit{T} = all of the single tokens \textit{t}
	\end{tcolorbox}
	
	\textit{k} terms \textit{t}, where every \textit{t} represents a distinct term occurring in \textit{D}. 
	\item \textbf{Stop word removal}: The most commonly used words in every language are stored in a so-called stop-word-table. Words from that table occurring in a document \textit{D} are removed. Example words are 'a', 'an' or 'the'. As already mentioned in the single-document summarization
	\item \textbf{Stemming}: converts words back into the base form. For example (houses -> house, running -> run). This is done to avoid redundancy.
\end{itemize}
 
 
After preprocessing the documents into word form, weights are computed to get an informative sentence score. This score is calculated for every sentence accordingly and is used as the input for a chosen optimization algorithm. This happens in the \textit{Extraction} box of Figure \ref{multi}. Like for the single-document, there are multiple methods to calculate extraction weights:

\begin{itemize}
	\item Abstraction and Information Fusion
	\item Topic-driven summarization
	\item Graph Spreading Activation
\end{itemize}

\textbf{Abstraction and Information Fusion} contains two steps. At first, as in most methods, a similarity measurement on word or sentence level is computed. When using an abstractive approach (Section \ref{ss:exabs}), the TFIDF score is commonly used. TFIDF is an information retrieval technique that weighs a term’s frequency (TF) and its inverse document frequency (IDF). Every term has its respective TF and IDF score. The product of the TF and IDF scores of a term is called the TFIDF weight of that term \cite{Ramos_usingtf-idf}. For each term, a vector is calculated that represents matches on the different features. Decision rules that were trained and learned from the data are used to classify each pair of text as either similar or dissimilar. This vector further feeds a subsequent algorithm that imputes the most related terms in the same topic-theme \cite{Das07asurvey} (Page 13). Once these scores and vectors are computed, the second step \textit{information fusion} starts. This step decides which information should be used for generating the final summary. Rather than just selecting a sentence that holds as a group representative, an algorithm that compares predicated argument structures of the terms. Within each topic, the algorithm needs to determine which terms are used and repeated often enough to be included in the summary.

\textbf{Topic-driven summarization} techniques aim to detect words that describe the topic of the multiple input documents. An advance of the initial idea of Luhn (proposed in Section \ref{ss:sd}) was to use the log-likelihood ratio test to identify particular words known as the \textit{topic signatures} \cite{textmining1958}. The log-likelihood is often used in statistical approaches for text summarization. 
There are two ways to calculate the importance of a sentence. The first way is as a function that contains the number of its topic signatures. Secondly, it can be calculated as the proportion of all the topic signatures in each sentence, respectively. While the first method usually gives higher scores to longer sentences, the second approach measures the occurrences of the topic words \cite{dunning-1993-accurate}.

\textbf{Graph Spreading Activation} is a similar approach to the graph-based approach from the single-document summarization. The PAGERANK algorithm can be used for this approach or other alternations like LexRank and TextRank. 


\subsection{Purpose}

\textbf{Types of Summaries}

The \textit{informative summary} contains the informative part of the original text. The main ideas from the text should be transmitted; for example, the abstract of research articles, where authors try to present the essential core of the research, is an informative summary. On the other hand, an indicative summary tries to transmit the relevant contents of the original document in such a way so that the readers can choose documents that match their interests to read further.

An \textit{indicative summary} is not meant to be a substitute for the original document. The opposite is the informative summary, which can replace the original documents as far as the crucial contents are concerned and by how much it was shortened down.

The \textit{keyword summary} tries to summarize the text into only keywords. Words will be weighted by their importance, and the most crucial ones are selected without caring for the grammar.

The \textit{headline summary} is the type of summary from the case study in Section \ref{ss:case}. The entire text gets compressed into a single sentence. It is a single line summary.

\textbf{Generic vs. user-oriented}

Generic systems generate summaries that consider all of the given information from the documents. On the other hand, user-oriented systems produce personalized summaries that concentrate on specific information from the original documents only. For example, the user-oriented news could only summarize the conservative or right-wing news if someone only searches for right-wing content.

\textbf{General purpose vs. domain-specific}

General-purpose summarizers can be used across any domains with barely any modification needed. On the contrary, domain-specific systems are programmed to process documents for a specific domain, like the research, news, or book summarizing domain.

\subsection{Output}\label{ss:exabs}

During this thesis, I have already mentioned the two core differences between text summarization: the extractive and abstractive approach. I use the definition of See et al. from 2017, where he regards the extractive summarizer as an explicit selection method for text snippets inside the single- or multi-document. While the abstractive summarizer generates new text snippets to describe summaries from a higher point of view by using vocabulary not included in the source input \cite{see-etal-2017-get} (Pages 1073-1083). 

\begin{figure}
	\begin{center}
		\includegraphics[width=5in]{photos/abex}\\
		\caption{Simplified Abstraction Extraction process}\label{abex}
	\end{center}
\end{figure}

Figure \ref{abex} shows the general model pipeline for either the extractive or abstractive approach. There is also a relatively new combinational approach, which will be explained in Section \ref{ss:trends} (\textit{Advanced Approaches for Text Summarization}). The topic input into the automatic text summarizer has already been explained in the multi-document Section \ref{ss:multi}. The parameter input contains, for example, the compression rate \(\tau\). This could be a value like 15\%, which means that the length of the original input document will be reduced by 85\%.

\subsubsection{Extractive}\label{ss:extractive}
	
The extractive approach aims to give an overview of the source document. This is done by selection fragments of the text (words, sentences, or paragraphs) that contain the essential information of the input text or texts. It can be seen as a copy and paste action for the most important fragments. Figure \ref{extract} shows the basic structure of an extractive summarizer, where the selection part is one of the three categorical types. The extractive approach has still some valid use cases, because:

\begin{itemize}
	\item \textbf{Pros}: The approach is robust because it uses existing natural-language phrases that are taken directly from the input.
	\item \textbf{Cons}: It lacks in flexibility since it cannot use original words or connectors. Furthermore, it cannot paraphrase as a human could do it. Sometimes the approach even applied wrong grammar.
\end{itemize}

According to Radev et av. in the 2002, extractive text summarization can be categorized as three different types \cite{ts-intro} (Pages 399-408):

\begin{itemize}
	\item Surface-level
	\item Intermediate-level
	\item Deep parsing techniques
\end{itemize}

\begin{figure}
	\begin{center}
		\includegraphics[width=4in]{photos/extract}\\
		\caption{General architecture of an extraction-based summarization system \cite{juan} (Page 31)}\label{extract}
	\end{center}
\end{figure}


The \textbf{surface-level} algorithms scratch on the right linguistic parts of the text. It cannot detect deep connections and abstractive features. It instead uses certain linguistic elements to detect the essential segments of a document \cite{juan} (Page 32). The mentioned inventor of automatic text summarizer Luhn \cite{textmining1958} used surface-level techniques to weight the occurrences of words in sentences. This technique is useful for a headline or keyword summarization. 

The \textbf{intermediate-level} categorization digs deeper into the meaning of specific paragraphs. It uses linguistic information to find relations between lexical-semantic sequences. This approach is more appropriate for extractive text summary than the surface-level approach.

The \textbf{deep parsing techniques} approaches make use of deep linguistic techniques that exploit the discursive structure of the input document or documents. It can find good relations in the text, and Marcu published one of the earliest methods in the year 2002 \cite{marcu}. He split the text into discursive units and uses a minimal set of relations, called discourse segmentation. An algorithm weighs and orders the elements accordingly. The highest weighted elements will be selected for the summary.

Even though I proposed in Section \ref{ss:input} for the single-document and multi-document different methods to compute the weights for the words, the extractive approach and the abstractive approach can be applied for both single- and multi-documents. The most common methods for the extractive approach to calculate weights are:

\begin{itemize}
	\item Graph-based (both single-document and multi-document) like PAGERANK and Textrank
	\item Luhn's algorithm \cite{textmining1958}, 
	\item Topic-driven (Section \ref{ss:multi}) 
	\item Neural Network approach (Section \ref{ss:neuralgen} - \textit{Advanced Approaches for Text Summarization})
\end{itemize}

\subsubsection{Abstractive}\label{ss:abstractive}

The abstractive method for automatic text summarization is one of the latest achievements in the research for useful automated summaries. When a system is based on an abstractive method, it means that it profoundly understands the text and seeks to generate grammatically correct and human-like coherent sentences. One of the first approaches was the \textit{FRUMP} system. It was one of the first systems which used semantic representation for the input text to generate summaries. This system was built in 1982 by Gerald Francis DeJong \cite{Dej82} (Pages 149-176).


The core features of FRUMP's algorithm were a complex architecture to understand documents written in natural language, and it had a hard-coded so-called knowledge structure \cite{Dej82} to simulate human behavior. However, the module for understanding the input was still conducted by a rather superficial analysis of the text. Frump's algorithm still made many mistakes. Taking into account the pros and cons from the extractive approach, the abstractive approach has different strengths and weaknesses:

\begin{itemize}
	\item \textbf{Pros}: Generates summaries in a more fluent way, by using words which are not included in the original input documents
	\item \textbf{Cons}: It is also a much more complex problem for the model to generate coherent phrases and connectors.
\end{itemize}


Nowadays, both the extractive and abstractive methods still are in use. It only depends on the to achieving task what to approach to chose. As I pointed out, the abstractive approach is more complex to use and requires more computational power. There are even ways to collaborate with the human, as in the \textbf{Aided Summarization}:

\begin{itemize}
	\item Combines automatic methods with human input.
	\item The algorithm suggests important information from the input document or documents, and the human decides whether to use it or not. It uses information retrieval and text mining.
\end{itemize}

One of the pioneers for abstractive summarization, Banko et al. (2000), recommends to use a machine translation model for the abstractive summarization model \cite{banko-etal-2000-headline} (Pages 318-325). I mentioned at the beginning of the thesis that sometimes innovative approaches for one discipline, for example, machine translation can benefit from other disciplines (text summarization). A machine translation model converts a source language into the translating target language, and the text summarization system converts a source document or documents into a target summary.

The most common methods for the abstractive approach are:

\begin{itemize}
	\item The encoder-decoder model (Section \ref{ss:encdec} - \textit{Advanced Approaches for Text Summarization})
	\item A neural attention model (Section \ref{ss:atten} - \textit{Advanced Approaches for Text Summarization})
	\item A sequence to sequence approach (Section \ref{ss:seq2seq} - \textit{Advanced Approaches for Text Summarization})
\end{itemize}

The \textbf{encoder-decoder} model from Section \ref{ss:encdec} (Encoder-Decoder) was originally invented for the use of machine translation. 
This model is also commonly used as an abstractive approach for text summarization. The text summarization model, which used the encoder-decoder model achieved state of the art results on the two sentence-level summarization datasets \textit{DUC-2004} and \textit{Gigaword}. Those two datasets are commonly used for evaluating the results of summarization in a standardized way. This will be explained in the next section.
The encoder-decoder model still faces some problems when using it directly for text summarization. It needs to be modified to work properly:

\begin{itemize}
	\item Set the focus on the important sentences and keywords.
	\item Handle the new, rare, or made-up words in the source document.
	\item Handle a long document.
	\item Make a more human-readable summary.
	\item Use a large vocabulary.
\end{itemize}


A \textbf{neural attention} model for sentence summarization is another applied abstractive method for text summary. I already explained how an attention model is built up in Section \ref{ss:atten}. To overcome issues, the attention model needs to take care of the following points:

\begin{itemize}
	\item Set the focus on the important sentences and keywords, like the encoder-decoder, since attention is encoder-decoder based too.
	\item Handle the novel, rare or made-up words in the source document as well. 
	\item Use beam-search to generate summary
	\item Use the n-gram match term as the loss function
\end{itemize}

Beam-search is an approximate search strategy to choose the best possible results from all available \textit{candidates} \cite{beam} (Pages 174-178). 

N-grams are groups of \textit{N} consecutive words that can be extracted from
a sentence. It can also be applied to characters instead of words. For example \cite{py} (Page 181):

\begin{tcolorbox}
	\begin{center}
		\textit{Mario Götze scored 3 times within 51 minutes} \\ 
	\end{center}
	
	It can be decomposed into the 2-gram:\\
	
	('Mario', 'Mario Götze', 'Götze', 'Götze scored', 'scored', 'scored 3', '3', '3 times', 'times',
	'times within', 'within', 'within 51', '51', '51 minutes', 'minutes')
\end{tcolorbox}

Relations of words can be better concluded out of the n-gram method.


The \textbf{Sequence-to-Sequence Recurrent Neural Networks} for abstractive Text Summarization. As with the previous two methods, this method has also already been introduced in Section \ref{ss:seq2seq} and Section \ref{ss:rnn}. This model uses slightly different approaches and faces, therefore partly the same, but more issues:

\begin{itemize}
	\item Set the focus on the important sentences abd keywords, like the encoder-decoder, since sequence to sequence models are encoder-decoder based too.
	\item Add enhanced features like named entity tags from Section \ref{ss:ref} or TFIDF scores from Section \ref{ss:multi}. 
	\item Use a large vocabulary
	\item Use a subset of pre-trained models with a vocabulary \cite{jean-etal-2015-using} (Pages 1-10).
\end{itemize}

All of these methods achieve better results than the extractive approaches, but there are still even more advanced approaches possible (Section \ref{ss:trends}) that are based on the abstractive models.  

\subsection{Evaluation}\label{ss:ev}

Evaluating automatically generated summaries has always been a challenging task. New methods for evaluation were specially created for the summarization discipline. It is divided basically into two different approaches to measure the quality of the generated summary \cite{Jones98automaticsummarising:} (Pages 1-12):

\begin{itemize}
	\item The \textbf{intrinsic evaluation} directly evaluates the output of a summarized text. 
	\item The \textbf{extrinsic evaluation} evaluates summaries based on their performance of the down-stream tasks that the generated summary was computed for.
\end{itemize}

\textbf{Intrinsic Evaluation}

Until today, there is no single best summarization evaluation method. The manual by hand evaluation is too expensive, as stated out by Lin in 2004 \cite{lin-2004-rouge} (Pages 74-81). For that reason, he published a method for a cheap automated evaluation metric \textit{ROUGE}. Nowadays, this approach is often coupled together with additional human ratings for the best result. ROGUE stands for Recall-Oriented Understudy for Gisting Evaluation. It is a set of methods that can automatically determine the quality of a generated summary by comparing it to a professional human-created summary for the same input text. The three most widely ROUGE based methods are \cite{yue} (Pages 2-3):

\begin{itemize}
	\item \textbf{ROUGE-N}: it calculates the percentage of overlapped n-grams with the reference summaries. It requires the one by one matches of all the words in n-grams. It compares the reference and generated summary, therefore, one by one. The number of words \textit{n} needs to be predefined
	\item \textbf{ROUGE-L}: it calculates the amount of the most one by one identical words. Hence it automatically identifies the longest in-sequence word overlapping without \textit{n} being predefined.
	\item \textbf{ROUGE-SU}: is not as straight forward as the other two. It measures the percentage of skip-bigrams (2-grams) and unigrams (1-grams), which overlap. When applying skip-bigrams without constraints on the distance between the words, it usually produces incorrect bigram matches. For that reason, the skip distances are limited by a certain number like 4 (ROUGE-SU4) \cite{lin-2004-rouge}.
\end{itemize}

An more advanced evaluation metric called \textbf{Pyramid} was also published by Nenkova and Passonneau in 2004 \cite{nenkova-passonneau-2004-evaluating} (Pages 145-152). Based on the assumption that there is no single best summary, but a variety of summaries can represent the input document in the same quality, Pyramid tries to evaluate summaries based on semantically matching content units. 

Another well-known evaluation approach originated from machine translation is \textbf{BLEU}, which means Bilingual Evaluation Understudy. BLEU can also be applied for evaluating text summaries. In general, BLEU is calculated on the n-gram co-occurrence between the generated summary and the ideal human-written summary. It measures how many of the words or n-grams in the machine-generated summary appeared in the reference summary from a human. Not to be mixed up with ROGUE, which counts how many of the words or n-grams in the human reference summary appeared in the machine-generated summary. Both approaches are relatively similar and can be used for evaluating, but they are not the same.

\textbf{Extrinsic Evaluation}

This approach has three most commonly methods, namely \cite{journals/cai/SteinbergerJ09} (Pages 10-11):

\begin{itemize}
	\item \textbf{Document categorization}: its suitability can measure the quality of a summary for surrogating a full document for categorization. This means, is the summary categorizes in the correct way? For example, is the summary of the topic of international news? 
	\item \textbf{Information retrieval}: a summary should capture the core points of a document, then an information retrieval machine indexed on a set of summaries should generate a good summary. 
	\item \textbf{Question Answering}: multiple choice with a single answer to be selected should measure how many of the questions randomly chosen people answered correctly under different conditions based on the generated summary.
\end{itemize}

\section{Advanced Approaches for Text Summarization}\label{ss:trends}

Since deep learning received more and more attraction, neural-based summarizers had a considerable influence on automatic summarization. Compared to the traditional models (extractive and partly abstractive models), neural-based models achieve better performance by relying less on human intervention if the training data is big enough.

\begin{figure}
	\begin{center}
		\includegraphics[width=5.5in]{photos/summ}\\
		\caption{Research fields with influence on the development of text summarization}\label{summ}
	\end{center}
\end{figure}

The four white boxes in Figure \ref{summ} have been introduced throughout this thesis. The next step to an even better (human-like) summary is the use of neural text generation, the state of the art approach. In the following, combinational approaches and the reinforcement approach will be introduced. Those methods rely entirely on neural networks (neural text generation) but achieve currently in 2020, the best state of the art results. Neural-based approaches are promising for text summarization in terms of the generated human-like summary when large data sets are available for training. Still, many challenges and issues with neural-based models remain unsolved. Future research directions such as the combinational approach or even adding the reinforcement learning are still in research.

\subsection{Combinational Approach}\label{ss:neuralgen}

\begin{itemize}
	\item \textbf{Pointer-Generator Network}
	\item \textbf{Extract then Abstract model}
\end{itemize}

The \textbf{Pointer Generator Network} proposed from See, Manning, and Liu in 2017 \cite{see-etal-2017-get} make use of an attention-based distribution to generate a probability. 
\begin{figure}
	\begin{center}
		\includegraphics[width=6in]{photos/pointer}\\
		\caption{The image describes the combination of the weights and the vocabulary distribution \cite{see-etal-2017-get} }\label{pointer}
	\end{center}
\end{figure}

Figure \ref{pointer} is based on the attention model (Figure \ref{attention1} from Section \ref{ss:atten}). 
This model switches the decoder(generator) and the pointer network by a probability \(p_{gen}\).
Furthermore it combines the vocabulary distribution and attention with \(p_{gen}\) and the (1 - \(p_{gen}\)) weight (Figure \ref{pointer}). The reason for choosing this probability multiplications exceed this bachelor thesis. The point for me to include it is to introduce the current state of the art models.

The \textbf{extract then abstract model} uses the extractive model first to select the sentence from a document or documents, and then secondly, it adopts the abstractive model to the selected sentences. 
Basically saying, the model combines a query focused extractive and abstractive model. It extracts the meaningful sentences with an extractive approach and calculates then the relevance score for each word, according to the query uses that as input for a pre-trained abstractive model.

\subsection{Transfer Learning}\label{ss:neuralgen2}

It is practical to reuse already learned models to make a new text summarization model. That is called \textit{Transfer Learning}. It allows to build a model by a tiny amount of new data and in a short period of time. This feature leads to domain-specific summarization.

\textbf{BERT} is the representative model that enables getting a good representation of sentences. Several methods are proposed to create summaries by BERT.

\textit{Fine-tune BERT for Extractive Summarization}

\begin{itemize}
	\item Use the pretrained models
	\item Get good sentence representation.
	\item BERT generates token-based features. Therefore there is a need to convert token-based features to sentence based representation. Liu and Lapata (2019) \cite{liu-lapata-2019-text} use \textit{Segmentation Embedding} to notice sentence boundaries and use the first token of the segmentation as the sentence embedding.
\end{itemize}

\textit{Pretraining-Based Natural Language Generation for Text Summarization}

\begin{itemize}
	\item How to use the pre-trained model?
	\item Getting good sentence representation and refine a generated sentence.
	\item BERT is trained to predict the masked token, so this approach can not generate a sequence. Haoyu et al. \cite{bert} use this method to generate the first summarization by an ordinary transformer model and then drop some tokens to let it fill by the BERT algorithm. The final summarization is created by the so-called input BERT representation and refined sentence representation made by BERT as well.
\end{itemize}
